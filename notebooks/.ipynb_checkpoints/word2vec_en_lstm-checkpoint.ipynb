{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9cba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 23:49:54.980434: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from my_nlp_module.preprocessing import preprocess_bbc_to_dict, PrepOption\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "label_to_class = {\n",
    "    \"business\": 0,\n",
    "    \"entertainment\": 1,\n",
    "    \"politics\": 2,\n",
    "    \"sport\": 3,\n",
    "    \"tech\": 4\n",
    "}\n",
    "\n",
    "class_to_label = {\n",
    "    0: \"business\",\n",
    "    1: \"entertainment\",\n",
    "    2: \"politics\",\n",
    "    3: \"sport\",\n",
    "    4: \"tech\"\n",
    "}\n",
    "\n",
    "path_to_model = \"../pretrained_models/40/model.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "embed_dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7739ee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't read 1 files:\n",
      "/Users/wojciechzyla/Desktop/AGH/praca_inz/projekt_inzynierski/bbc-text/train/sport/199.txt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    459\n",
       "1    347\n",
       "2    375\n",
       "3    458\n",
       "4    360\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_path = \"../datasets/bbc-text/train\"\n",
    "dataset_test_path = \"../datasets/bbc-text/test\"\n",
    "\n",
    "options = [PrepOption.INTERPUNCTION, PrepOption.RUBBISH, PrepOption.NUMBERS, PrepOption.STOPWORDS]\n",
    "preprocessed = preprocess_bbc_to_dict(dataset_train_path, options)\n",
    "\n",
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for key in preprocessed.keys():\n",
    "    for doc in preprocessed[key]:\n",
    "        documents.append(doc)\n",
    "        labels.append(label_to_class[key])\n",
    "        \n",
    "df = pd.DataFrame({\"document\": documents, \"label\": labels})\n",
    "df = df.sample(frac=1, axis=0).reset_index(drop=True)\n",
    "\n",
    "df.groupby(['label'])['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f18ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 8000\n",
    "tok = Tokenizer(MAX_WORDS)\n",
    "tok.fit(list(df['document']))\n",
    "vocab_size = len(tok.vocab) + 1\n",
    "\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.vocab.items():\n",
    "    try:\n",
    "        embed_vector=model[word]\n",
    "        embed_matrix[i]=embed_vector\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b8e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length of document is 530\n"
     ]
    }
   ],
   "source": [
    "encoded = tok.texts_to_sequences(list(df['document']))\n",
    "max_doc_len=-1\n",
    "for doc in encoded:\n",
    "    if(len(doc)>max_doc_len):\n",
    "        max_doc_len=len(doc)\n",
    "print(f\"Maximal length of document is {max_doc_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd255fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded documents array: (1999, 530)\n"
     ]
    }
   ],
   "source": [
    "pad_docs = pad_sequences(encoded, maxlen=max_doc_len, padding='post')\n",
    "print(f\"Shape of padded documents array: {pad_docs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5551148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 23:50:40.354089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "Y=df['label']\n",
    "x_train,x_test,y_train,y_test=train_test_split(pad_docs,Y,test_size=0.20,random_state=42)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9748168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 530, 100)          800100    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                10836     \n",
      "                                                                 \n",
      " classifier (Dense)          (None, 5)                 425       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 895,841\n",
      "Trainable params: 95,741\n",
      "Non-trainable params: 800,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier=tf.keras.models.Sequential()\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,\n",
    "                                      input_length=max_doc_len, weights=[embed_matrix],\n",
    "                                      trainable=False)\n",
    "classifier.add(embedding)\n",
    "\n",
    "classifier.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "classifier.add(tf.keras.layers.Dense(84, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax, name='classifier'))\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "\"\"\"steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "      initial_learning_rate=init_lr,\n",
    "      decay_steps=num_train_steps,\n",
    "      end_learning_rate=0.0)\n",
    "\n",
    "opt = tf.keras.optimizers.experimental.AdamW(learning_rate=lr_schedule)\"\"\"\n",
    "\n",
    "classifier.compile(optimizer='adam',\n",
    "                        loss=loss,\n",
    "                        metrics=metrics)\n",
    "\n",
    "\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2064e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 14s 221ms/step - loss: 1.1584 - sparse_categorical_accuracy: 0.5247 - val_loss: 0.5572 - val_sparse_categorical_accuracy: 0.8300\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 11s 211ms/step - loss: 0.5490 - sparse_categorical_accuracy: 0.8218 - val_loss: 0.4079 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 11s 212ms/step - loss: 0.3720 - sparse_categorical_accuracy: 0.8755 - val_loss: 0.3486 - val_sparse_categorical_accuracy: 0.8675\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 11s 216ms/step - loss: 0.3395 - sparse_categorical_accuracy: 0.8937 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.8875\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 0.3008 - sparse_categorical_accuracy: 0.9087 - val_loss: 0.2848 - val_sparse_categorical_accuracy: 0.9075\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 0.2500 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.2932 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 11s 216ms/step - loss: 0.2386 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.4522 - val_sparse_categorical_accuracy: 0.8625\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 11s 216ms/step - loss: 0.2301 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.3174 - val_sparse_categorical_accuracy: 0.8825\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 11s 219ms/step - loss: 0.2445 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.2969 - val_sparse_categorical_accuracy: 0.9075\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 11s 216ms/step - loss: 0.2127 - sparse_categorical_accuracy: 0.9306 - val_loss: 0.3519 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 11s 213ms/step - loss: 0.1926 - sparse_categorical_accuracy: 0.9356 - val_loss: 0.2158 - val_sparse_categorical_accuracy: 0.9225\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 11s 213ms/step - loss: 0.1585 - sparse_categorical_accuracy: 0.9512 - val_loss: 0.2785 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 13/20\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1628 - sparse_categorical_accuracy: 0.9466"
     ]
    }
   ],
   "source": [
    "# free memory\n",
    "del model\n",
    "del preprocessed\n",
    "del df\n",
    "del pad_docs\n",
    "gc.collect()\n",
    "\n",
    "classifier.fit(x_train,y_train,epochs=epochs,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d4b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
